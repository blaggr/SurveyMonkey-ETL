{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from surveymonkey.exceptions import UnknownError, BadRequestError, AuthorizationError, PermissionError, \\\n",
    "    ResourceNotFoundError, ResourceConflictError, RequestEntityTooLargeError, RateLimitReachedError, \\\n",
    "    InternalServerError, UserSoftDeletedError, UserDeletedError\n",
    "\n",
    "'''\n",
    "Token expiration and revocation\n",
    "Our access tokens don’t currently expire but may in the future. We’ll warn all developers before making changes.\n",
    "Access tokens can be revoked by the user. If this happens, you’ll get a JSON-encoded response body including a key \n",
    "statuswith a value of 1 and a key errmsg with the value of Client revoked access grant when making an API request. \n",
    "If you get this response, you’ll need to complete OAuth again.\n",
    "'''\n",
    "\n",
    "BASE_URL = \"https://api.surveymonkey.com\"\n",
    "API_URL = \"https://api.surveymonkey.com/v3\"\n",
    "AUTH_CODE = \"/oauth/authorize\"\n",
    "ACCESS_TOKEN_URL = \"/oauth/token\"\n",
    "\n",
    "\n",
    "class Client(object):\n",
    "\n",
    "    def __init__(self, client_id=None, client_secret=None, redirect_uri=None, access_token=None):\n",
    "        self.code = None\n",
    "        self.client_id = client_id\n",
    "        self.redirect_uri = redirect_uri\n",
    "        self.client_secret = client_secret\n",
    "        self._access_token = access_token\n",
    "\n",
    "    def get_survey_lists(self):\n",
    "        \"\"\"\n",
    "        List all created surveys.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        endpoint = \"/surveys?per_page=100\"\n",
    "        url = API_URL + endpoint\n",
    "        return self._get(url)\n",
    "\n",
    "    def get_survey_details(self, survey_id):\n",
    "        \"\"\"\n",
    "        Details of a specific survey\n",
    "        :param survey_id: id of specific survey to get details from\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        endpoint = f\"/surveys/{survey_id}/details\"\n",
    "        url = API_URL + endpoint\n",
    "        return self._get(url)\n",
    "\n",
    "    def get_survey_response_bulk(self, survey_id, page):\n",
    "        \"\"\"\n",
    "        Retrieves a list of full expanded responses, including answers to all questions.\n",
    "        :param survey_id: id of survey\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        endpoint = f\"/surveys/{survey_id}/responses/bulk?page={page}&per_page=100\"\n",
    "        url = API_URL + endpoint\n",
    "        return self._get(url)\n",
    "\n",
    "\n",
    "    # Communications\n",
    "    def _get(self, endpoint, **kwargs):\n",
    "        return self._request('GET', endpoint, **kwargs)\n",
    "\n",
    "    def _post(self, endpoint, **kwargs):\n",
    "        return self._request('POST', endpoint, **kwargs)\n",
    "\n",
    "    def _put(self, endpoint, **kwargs):\n",
    "        return self._request('PUT', endpoint, **kwargs)\n",
    "\n",
    "    def _patch(self, endpoint, **kwargs):\n",
    "        return self._request('PATCH', endpoint, **kwargs)\n",
    "\n",
    "    def _delete(self, endpoint, **kwargs):\n",
    "        return self._request('DELETE', endpoint, **kwargs)\n",
    "\n",
    "    def _request(self, method, url, **kwargs):\n",
    "        _headers = {\"Authorization\": \"Bearer %s\" % self._access_token, \"Content-Type\": \"application/json\"}\n",
    "        return self._parse(requests.request(method, url, headers=_headers, **kwargs))\n",
    "\n",
    "    def _parse(self, response):\n",
    "        status_code = response.status_code\n",
    "        if 'application/json' in response.headers['Content-Type']:\n",
    "            r = response.json()\n",
    "        else:\n",
    "            r = response.text\n",
    "        if \"error\" in r:\n",
    "            self.get_error(dict(r))\n",
    "        if \"error\" not in r and status_code not in [200, 201, 204]:\n",
    "            raise UnknownError()\n",
    "        if status_code in (200, 201):\n",
    "            return r\n",
    "        if status_code == 204:\n",
    "            return None\n",
    "\n",
    "    def get_error(self, error):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        error_code = error['error']\n",
    "        error_message = error['message']\n",
    "        if error_code == \"1000\":\n",
    "            raise BadRequestError(error_message)\n",
    "        elif error_code == \"1001\":\n",
    "            raise BadRequestError(error_message)\n",
    "        elif error_code == \"1002\":\n",
    "            raise BadRequestError(error_message)\n",
    "        elif error_code == \"1003\":\n",
    "            raise BadRequestError(error_message)\n",
    "        elif error_code == \"1004\":\n",
    "            raise BadRequestError(error_message)\n",
    "        elif error_code == \"1010\":\n",
    "            raise AuthorizationError(error_message)\n",
    "        elif error_code == \"1011\":\n",
    "            raise AuthorizationError(error_message)\n",
    "        elif error_code == \"1012\":\n",
    "            raise AuthorizationError(error_message)\n",
    "        elif error_code == \"1013\":\n",
    "            raise AuthorizationError(error_message)\n",
    "        elif error_code == \"1014\":\n",
    "            raise PermissionError(error_message)\n",
    "        elif error_code == \"1015\":\n",
    "            raise PermissionError(error_message)\n",
    "        elif error_code == \"1016\":\n",
    "            raise PermissionError(error_message)\n",
    "        elif error_code == \"1017\":\n",
    "            raise PermissionError(error_message)\n",
    "        elif error_code == \"1018\":\n",
    "            raise PermissionError(error_message)\n",
    "        elif error_code == \"1020\":\n",
    "            raise ResourceNotFoundError(error_message)\n",
    "        elif error_code == \"1025\":\n",
    "            raise ResourceConflictError(error_message)\n",
    "        elif error_code == \"1026\":\n",
    "            raise ResourceConflictError(error_message)\n",
    "        elif error_code == \"1030\":\n",
    "            raise RequestEntityTooLargeError(error_message)\n",
    "        elif error_code == \"1040\":\n",
    "            raise RateLimitReachedError(error_message)\n",
    "        elif error_code == \"1050\":\n",
    "            raise InternalServerError(error_message)\n",
    "        elif error_code == \"1051\":\n",
    "            raise InternalServerError(error_message)\n",
    "        elif error_code == \"1052\":\n",
    "            raise UserSoftDeletedError(error_message)\n",
    "        elif error_code == \"1053\":\n",
    "            raise UserDeletedError(error_message)\n",
    "        else:\n",
    "            raise UnknownError(\"UNKNOWN ERROR: {}\".format(error['message']))\n",
    "            \n",
    "ACCESS_TOKEN = 'your access token'\n",
    "CLIENT_ID = 'your client id'\n",
    "CLIENT_SECRET = 'your client secret token'\n",
    "REDIRECT_URI = 'na'\n",
    "client=Client(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, redirect_uri=REDIRECT_URI, access_token=ACCESS_TOKEN)\n",
    "\n",
    "def get_headings(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a SurveyMonkey json object from an API call and creates a dataframe with the headers and the corresponding question_ids\n",
    "    \n",
    "    Arguments: a single json object created from a .get() call on the SurveyMonkey API\n",
    "    '''\n",
    "    \n",
    "    #import necessary packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    ids_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for ids in question['id']:\n",
    "                ids_list.append(ids)     \n",
    "    def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "        for i in range(0, len(l), n):  \n",
    "            yield l[i:i + n] \n",
    "  \n",
    "    # How many elements each \n",
    "    # list should have \n",
    "    n = 9\n",
    "  \n",
    "    x = list(divide_chunks(ids_list, n)) \n",
    "\n",
    "    #create question id dataframe from list of ids the join is joining together each list into one number, taking out commas and quotation marks\n",
    "    heading_ids = pd.DataFrame([''.join(i) for i in x])\n",
    "\n",
    "    headings_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for headings in question['headings']:\n",
    "                headings_list.append(headings)\n",
    "                headings = pd.DataFrame(headings_list)\n",
    "\n",
    "#concatenate the heading ids with the headings\n",
    "    questions_and_ids = pd.concat([headings, heading_ids], axis = 1).rename(columns = {0:'question_id'})\n",
    "    \n",
    "    return pd.DataFrame(questions_and_ids)\n",
    "\n",
    "def get_answers_and_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object from the SurveyMonkey API, flattens the json, subsets the items in the json, and extracts\n",
    "    the answers and their corresponding ids and then places all of this into a new dataframe\n",
    "    \n",
    "    Arguments: json object from SurveyMonkey API call\n",
    "    '''\n",
    "    \n",
    "    #flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    \n",
    "    #use the function on the json\n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {'index':'detail_buckets', 0:'details'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    #searching for all responses within the survey using regex\n",
    "    choices = flattened_details[flattened_details['detail_buckets'].str.contains(r'questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_text|questions_[0-9]{1,2}_answers_other_text')].rename(columns = {'details':'possible_choices'})\\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "    #searching for all response ids within the survey with regex\n",
    "    choice_ids = flattened_details[flattened_details['detail_buckets'].str.\\\n",
    "    contains(r'pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_id|pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_other_id')].drop('detail_buckets', axis = 1).rename(columns = {'details':'response_id'}).reset_index(drop = True)\n",
    "\n",
    "    final_df = pd.concat([choices, choice_ids], axis = 1)\n",
    "    \n",
    "    final_df.drop('detail_buckets', axis = 1, inplace = True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "#json = client.get_survey_response_bulk('186927358')\n",
    "#json_2 = client.get_survey_details('186927358')\n",
    "\n",
    "def get_personid_choiceid_rowid_surveyid(json, json_2):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    So, theres a lot going on here... basically this function takes a json object from the SurveyMonkey API call and produces all answers, \n",
    "    their corresponding row_ids (which are question ids in the surveys), the respondent ids, the question ids, choice ids, and text answers\n",
    "    \n",
    "    Arguments: you must create 2 json objects from the client call first, \n",
    "        json = bulk responses from API\n",
    "        json_2 = survey details\n",
    "        \n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #use function to flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    #getting the bulk responses and flattening the json file\n",
    "    bulk_responses = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    #renaming the columns in the dataframe\n",
    "    bulk_responses.rename(columns = {'index':'answer_type', 0:'answer'}, inplace = True)\n",
    "    \n",
    "    #searching for ids within the bulk responses, looks like I am only getting 50 unique back at a time...\n",
    "    bulk_responses = bulk_responses[bulk_responses['answer_type'].str.contains(r'text|data_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_row_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_choice_id|other')].reset_index(drop = True)\n",
    "    \n",
    "    #grabbing the survey details from json_2\n",
    "    survey_details = pd.Series(flatten_json_iterative_solution(json_2)).to_frame()\n",
    "    \n",
    "    #get survey id and create column denoting the survey id\n",
    "    bulk_responses['survey_id'] = survey_details[survey_details.index.str.contains(r'^id') == True][0][0]\n",
    "\n",
    "    #renameing the ids to normal names\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_id', value = 'respondent_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_text', \n",
    "                           value = 'text_answer', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_id', \n",
    "                           value = 'question_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_row_id', \n",
    "                           value = 'row_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_choice_id', \n",
    "                           value = 'choice_id', regex = True, inplace = True)\n",
    "    \n",
    "    #some surveys have \"other\" as an option, this covers those\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,4}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_other_id', \n",
    "                           value = 'other_id', regex = True, inplace = True)\n",
    "\n",
    "    #fill in survey_id column completely\n",
    "    bulk_responses.ffill(inplace = True)\n",
    "    \n",
    "    #create mask to use as a transfer from row ids to another column\n",
    "    mask = (bulk_responses['answer_type'] == 'row_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['row_id'] = bulk_responses['answer_type'][mask]\n",
    "\n",
    "    #set row id equal to the actual row id\n",
    "    bulk_responses.row_id[bulk_responses.row_id == 'row_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all row ids up one in order to drop the row ids from the details column\n",
    "    bulk_responses['row_id'] = bulk_responses['row_id'].shift(-1)\n",
    "\n",
    "    #drop all row ids rows from df so that row id is listed beside choice id\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('row_id')]\n",
    "\n",
    "    #rearrange columns\n",
    "    bulk_responses = bulk_responses[['answer_type', 'answer', 'row_id', 'survey_id']]\n",
    "    \n",
    "    #create mask to use as a transfer to question ids column\n",
    "    mask_2 = (bulk_responses['answer_type'] == 'question_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['question_id'] = bulk_responses['answer_type'][mask_2]\n",
    "\n",
    "    #put actual question ids into the question ids column\n",
    "    bulk_responses.question_id[bulk_responses.question_id == 'question_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all of them down 1 to make sure answers line up next to actual questions\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].shift(1)\n",
    "\n",
    "    #drop question id from answer type\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('question_id')]\n",
    "\n",
    "    #forward fill the question ids so that each row id has a corresponding question id\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].ffill()\n",
    "\n",
    "    mask_3 = (bulk_responses['answer_type'] == 'respondent_id')\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['answer_type'][mask_3]\n",
    "\n",
    "    bulk_responses.respondent_id[bulk_responses.respondent_id == 'respondent_id'] = bulk_responses.answer\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['respondent_id'].ffill()\n",
    "\n",
    "    bulk_responses = bulk_responses[~bulk_responses['answer_type'].str.contains('respondent_id')]\n",
    "    \n",
    "    #change column order\n",
    "    bulk_responses = bulk_responses[['respondent_id', 'survey_id', 'answer_type', 'answer', 'row_id', 'question_id']].reset_index(drop = True)\n",
    "    \n",
    "    return bulk_responses\n",
    "\n",
    "# = client.get_survey_details('186925932')\n",
    "\n",
    "def get_row_text_and_row_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object returned from a call to the SurveyMonkey API and returns sub-questions and their ids\n",
    "    \n",
    "    Argument: a json object called from the SurveyMonkey API\n",
    "    \n",
    "    Requirements: the SurveyMonkey client - https://github.com/GearPlug/surveymonkey-python\n",
    "    '''\n",
    "    \n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {0:'row_id'}, inplace = True)\n",
    "    \n",
    "    row_text = flattened_details[flattened_details['index'].str.contains(r'pages_[0-9]{1,4}_questions_[0-9]{1,4}_answers_rows_[0-9]{1,4}_text|rows_[0-9]{1,4}_id')] \n",
    "    \n",
    "    row_text_ids = row_text[row_text['row_id'].str.contains(r'^[0-9]{1,9}') == True]\n",
    "\n",
    "    row_id_df = pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,1].dropna())\n",
    "    \n",
    "    row_text_df =  pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,0].dropna())\n",
    "\n",
    "    rows_df = pd.concat([row_text_df, row_id_df], axis = 1)\n",
    "\n",
    "    rows_df.iloc[:,1] = rows_df.iloc[:,1].shift(-1)\n",
    "\n",
    "    rows_df.dropna(inplace = True)\n",
    "\n",
    "    rows_df.columns = ['row_text', 'row_id']\n",
    "    \n",
    "    return rows_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------get all surveys\n",
    "\n",
    "\n",
    "surveys = pd.Series(flatten_json_iterative_solution(client.get_survey_lists())).to_frame()\n",
    "surveys.reset_index(inplace = True)\n",
    "surveys.rename(columns = {'index':'detail_buckets', 0:'details'}, inplace = True)\n",
    "surveys = surveys[surveys['detail_buckets'].str.contains('data_[0-9]{1,20}_id|data_[0-9]{1,20}_title')]\n",
    "mask = surveys.details.str.contains(r'[A-Za-z]')\n",
    "surveys['survey_name'] = surveys['details'][mask]\n",
    "surveys['survey_name'] = surveys['survey_name'].shift(-1)\n",
    "surveys = surveys[~surveys['detail_buckets'].str.contains(r'data_[0-9]{1,20}_title')]\n",
    "surveys.rename(columns = {'details':'survey_id'}, inplace = True)\n",
    "surveys.drop('detail_buckets', axis = 1, inplace = True)\n",
    "surveys.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#--------------------------------------------------------------------------------get all headings\n",
    "\n",
    "\n",
    "all_headings = pd.concat([get_headings(client.get_survey_details(survey_id)) for survey_id in surveys['survey_id']])\n",
    "\n",
    "#-------------------------------------------------------------------------------get all possible responses\n",
    "\n",
    "\n",
    "every_single_possible_response = pd.concat([get_answers_and_ids(client.get_survey_details(survey_id)) for survey_id in surveys['survey_id']])\n",
    "\n",
    "#----------------------------------------------------------------------------------get all subquestions\n",
    "\n",
    "\n",
    "every_single_subquestion_info = pd.concat([get_row_text_and_row_ids(client.get_survey_details(survey_id)) for survey_id in surveys['survey_id']])\n",
    "\n",
    "#-------------------------------------------------------------------------------------getting all actual responses\n",
    "\n",
    "#looping through every single survey and, depending on the number of responses to that survey, looping through each subsequent page of responses\n",
    "# and grabbing every single answer from that survey\n",
    "\n",
    "\n",
    "all_responses = []\n",
    "for survey_id in surveys['survey_id']:\n",
    "    if client.get_survey_details(survey_id)['response_count'] <= 100:\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(101, 200):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(201, 300):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(301, 400):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(401, 500):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 5), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(501, 600):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 5), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 6), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(601, 700):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 5), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 6), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 7), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(701, 800):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 5), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 6), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 7), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 8), client.get_survey_details(survey_id)))\n",
    "    elif client.get_survey_details(survey_id)['response_count'] in range(801, 900):\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 1), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 2), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 3), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 4), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 5), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 6), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 7), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 8), client.get_survey_details(survey_id)))\n",
    "        all_responses.append(get_personid_choiceid_rowid_surveyid(client.get_survey_response_bulk(survey_id, 9), client.get_survey_details(survey_id)))\n",
    "\n",
    "#concatenate the list into one dataframe\n",
    "all_responses = pd.concat(all_responses)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------connect to SQL\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "#create the engine\n",
    "db_connection_str = 'mysql+pymysql://[your username (usually root)]:[your password]@[your hostname (usually 127.0.0.1)]/[schema that you want to connect to/send df into]'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------clean dfs for SQL export\n",
    "\n",
    "all_headings = all_headings[['question_id', 'heading']]\n",
    "#clear out anything but numbers and letters\n",
    "all_headings['heading'] = [re.sub(r'\\W+', ' ', str(question)) for question in every_single_heading['heading']]\n",
    "\n",
    "all_responses.rename(columns = {'index':'rID'}, inplace = True)\n",
    "#clear out anything but numbers and letters\n",
    "all_responses['answer'] = [re.sub(r'\\W+', ' ', str(answer)) for answer in all_responses['answer']]\n",
    "\n",
    "#-----------------------------------------------------------------------------------send it all to SQL (you must set up your schema in workbench first)\n",
    "# all_responses.to_sql('responses', con = db_connection, if_exists = 'replace', index = False)\n",
    "# surveys.to_sql('surveys', con = db_connection, if_exists = 'replace', index = False)\n",
    "# every_single_heading.to_sql('questions', con = db_connection, if_exists = 'replace', index = False)\n",
    "# every_single_possible_response.to_sql('possible_responses', con = db_connection, if_exists = 'replace', index = False)\n",
    "# every_single_subquestion_info.to_sql('subquestions', con = db_connection, if_exists = 'replace', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
